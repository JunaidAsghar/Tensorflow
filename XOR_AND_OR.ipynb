{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : LOSS 2.7743\n",
      "Epoch 1 : LOSS 2.47719\n",
      "Epoch 2 : LOSS 2.3138\n",
      "Epoch 3 : LOSS 2.18257\n",
      "Epoch 4 : LOSS 2.06767\n",
      "Epoch 5 : LOSS 1.96512\n",
      "Epoch 6 : LOSS 1.87299\n",
      "Epoch 7 : LOSS 1.78985\n",
      "Epoch 8 : LOSS 1.71449\n",
      "Epoch 9 : LOSS 1.64591\n",
      "Epoch 10 : LOSS 1.58323\n",
      "Epoch 11 : LOSS 1.52573\n",
      "Epoch 12 : LOSS 1.47278\n",
      "Epoch 13 : LOSS 1.42384\n",
      "Epoch 14 : LOSS 1.37845\n",
      "Epoch 15 : LOSS 1.33623\n",
      "Epoch 16 : LOSS 1.29683\n",
      "Epoch 17 : LOSS 1.25997\n",
      "Epoch 18 : LOSS 1.22539\n",
      "Epoch 19 : LOSS 1.19287\n",
      "Epoch 20 : LOSS 1.16222\n",
      "Epoch 21 : LOSS 1.13327\n",
      "Epoch 22 : LOSS 1.10586\n",
      "Epoch 23 : LOSS 1.07988\n",
      "Epoch 24 : LOSS 1.0552\n",
      "Epoch 25 : LOSS 1.03171\n",
      "Epoch 26 : LOSS 1.00934\n",
      "accuracy 1.0\n",
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple Neural network/Model learning the AND function without any hidden layer \n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Trainig Data for AND function:\n",
    "x_train = [[0, 0], [0, 1], [1, 0], [1, 1]] # input\n",
    "# y_train=[0,      0,      0,      1]   # output =>\n",
    "y_true=[[1,0],  [1,0],  [1,0], [0,1]] # ONE HOT EN-CODING REPRESENTATION! 'class' [1,0]==0 [0,1]==1\n",
    "\n",
    "# Initializing Placeholders x and y_ for input and output data\n",
    "x = tf.placeholder(\"float\", [None,2])\n",
    "y_ = tf.placeholder(\"float\", [None, 2]) # two output classes\n",
    "\n",
    "# initializign weights \n",
    "Weights = tf.Variable(tf.random_uniform([2,2], -.1, .1))\n",
    "\n",
    "#initializing biases\n",
    "biases = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Linear model \n",
    "logits= tf.matmul(x, Weights) + biases\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy)\n",
    "\n",
    "# Training the model\n",
    "\n",
    "# Initializing tensorflw Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Initializing the global variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # feed the net with Trainig inputs and true ouptput class.\n",
    "    # Creaating the Feed_dict \n",
    "    feed_dict={x: x_train, y_:y_true } \n",
    "    loss,opt=sess.run([cross_entropy,train_step],feed_dict)\n",
    "    if loss<1:break # early stopping\n",
    "    print (\"Epoch %d : LOSS %s\" %(epoch,loss)) # loss should decrease over time)\n",
    "\n",
    "\n",
    "# Testing  the trained model by checkong the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_true,1)) # argmax along dim-1\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # [True, False, True, True] -> [1,0,1,1] -> 0.75.\n",
    "\n",
    "print (\"accuracy %s\" % (accuracy.eval({x: x_train, y_: y_true})))\n",
    "\n",
    "# Check the results on the trained model by passing new data\n",
    "output = tf.argmax(y,1) \n",
    "print ( output.eval({x: x_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : LOSS 2.77089\n",
      "Epoch 1 : LOSS 2.44198\n",
      "Epoch 2 : LOSS 2.3217\n",
      "Epoch 3 : LOSS 2.26993\n",
      "Epoch 4 : LOSS 2.24028\n",
      "Epoch 5 : LOSS 2.22059\n",
      "Epoch 6 : LOSS 2.20786\n",
      "Epoch 7 : LOSS 2.18876\n",
      "Epoch 8 : LOSS 2.16922\n",
      "Epoch 9 : LOSS 2.14606\n",
      "Epoch 10 : LOSS 2.1072\n",
      "Epoch 11 : LOSS 2.07484\n",
      "Epoch 12 : LOSS 2.00933\n",
      "Epoch 13 : LOSS 1.95651\n",
      "Epoch 14 : LOSS 1.86966\n",
      "Epoch 15 : LOSS 1.76552\n",
      "Epoch 16 : LOSS 1.67507\n",
      "Epoch 17 : LOSS 1.54481\n",
      "Epoch 18 : LOSS 1.42269\n",
      "Epoch 19 : LOSS 1.32723\n",
      "Epoch 20 : LOSS 1.21396\n",
      "Epoch 21 : LOSS 1.10193\n",
      "Epoch 22 : LOSS 1.0019\n",
      "accuracy 1.0\n",
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple Neural network/Model learning the AND function one hidden layer \n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data Dimenshions\n",
    "hidden_nodes_l1 = 30\n",
    "\n",
    "# Trainig Data for AND function:\n",
    "x_train = [[0, 0], [0, 1], [1, 0], [1, 1]] # input\n",
    "# y_train=[0,      0,      0,      1]   # output =>\n",
    "y_true=[[1,0],  [1,0],  [1,0], [0,1]] # ONE HOT EN-CODING REPRESENTATION! 'class' [1,0]==0 [0,1]==1\n",
    "\n",
    "# Initializing Placeholders x and y_ for input and output data\n",
    "x = tf.placeholder(\"float\", [None,2])\n",
    "y_ = tf.placeholder(\"float\", [None, 2]) # two output classes\n",
    "\n",
    "# First layer.\n",
    "# initializign weights \n",
    "weights_l1 = tf.Variable(tf.random_uniform([2, hidden_nodes_l1], -.01, .01))\n",
    "\n",
    "#initializing biases\n",
    "biases_l1 = tf.Variable(tf.random_uniform([hidden_nodes_l1], -.01, .01))\n",
    "layer_1  = tf.nn.relu(tf.matmul(x,weights_l1) + biases_l1) \n",
    "\n",
    "# Output Layer\n",
    "# initializign weights \n",
    "weights_out = tf.Variable(tf.random_uniform([hidden_nodes_l1,2], -.1, .1))\n",
    "\n",
    "#initializing biases\n",
    "biases_out = tf.Variable(tf.zeros([2]))\n",
    "logits = tf.matmul(layer_1, weights_out) + biases_out\n",
    "\n",
    "y = tf.nn.softmax(logits)\n",
    "# Define loss and optimizer\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy)\n",
    "\n",
    "# Training the model\n",
    "\n",
    "# Initializing tensorflw Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Initializing the global variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # feed the net with Trainig inputs and true ouptput class.\n",
    "    # Creaating the Feed_dict \n",
    "    feed_dict={x: x_train, y_:y_true } \n",
    "    loss,opt=sess.run([cross_entropy,train_step],feed_dict)\n",
    "    if loss<1:break # early stopping\n",
    "    print (\"Epoch %d : LOSS %s\" %(epoch,loss)) # loss should decrease over time)\n",
    "\n",
    "\n",
    "# Testing  the trained model by checkong the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_true,1)) # argmax along dim-1\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # [True, False, True, True] -> [1,0,1,1] -> 0.75.\n",
    "\n",
    "print (\"accuracy %s\" % (accuracy.eval({x: x_train, y_: y_true})))\n",
    "\n",
    "# Check the results on the trained model by passing new data\n",
    "output = tf.argmax(y,1) \n",
    "print ( output.eval({x: x_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0 : entropy 2.77236\n",
      "step 1 : entropy 2.77206\n",
      "step 2 : entropy 2.77143\n",
      "step 3 : entropy 2.77039\n",
      "step 4 : entropy 2.76981\n",
      "step 5 : entropy 2.76927\n",
      "step 6 : entropy 2.76774\n",
      "step 7 : entropy 2.76716\n",
      "step 8 : entropy 2.76601\n",
      "step 9 : entropy 2.76413\n",
      "step 10 : entropy 2.76292\n",
      "step 11 : entropy 2.76127\n",
      "step 12 : entropy 2.75817\n",
      "step 13 : entropy 2.75644\n",
      "step 14 : entropy 2.7534\n",
      "step 15 : entropy 2.74869\n",
      "step 16 : entropy 2.74585\n",
      "step 17 : entropy 2.74058\n",
      "step 18 : entropy 2.73347\n",
      "step 19 : entropy 2.7281\n",
      "step 20 : entropy 2.72045\n",
      "step 21 : entropy 2.70739\n",
      "step 22 : entropy 2.70162\n",
      "step 23 : entropy 2.6849\n",
      "step 24 : entropy 2.67041\n",
      "step 25 : entropy 2.65213\n",
      "step 26 : entropy 2.63783\n",
      "step 27 : entropy 2.59797\n",
      "step 28 : entropy 2.59489\n",
      "step 29 : entropy 2.53823\n",
      "step 30 : entropy 2.52345\n",
      "step 31 : entropy 2.44634\n",
      "step 32 : entropy 2.45438\n",
      "step 33 : entropy 2.37897\n",
      "step 34 : entropy 2.31524\n",
      "step 35 : entropy 2.27716\n",
      "step 36 : entropy 2.17192\n",
      "step 37 : entropy 2.15001\n",
      "step 38 : entropy 2.09467\n",
      "step 39 : entropy 1.93382\n",
      "step 40 : entropy 1.92118\n",
      "step 41 : entropy 1.89225\n",
      "step 42 : entropy 1.68072\n",
      "step 43 : entropy 1.64658\n",
      "step 44 : entropy 1.64887\n",
      "step 45 : entropy 1.45151\n",
      "step 46 : entropy 1.34464\n",
      "step 47 : entropy 1.29679\n",
      "step 48 : entropy 1.13562\n",
      "step 49 : entropy 1.13453\n",
      "step 50 : entropy 1.00111\n",
      "accuracy 1.0\n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env PYTHONIOENCODING=\"utf-8\" python\n",
    "\"\"\"\n",
    "A simple neural network learning the XOR function\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Desired input output mapping of XOR function:\n",
    "x_ = [[0, 0], [0, 1], [1, 0], [1, 1]] # input\n",
    "#labels=[0,      1,      1,      0]   # output =>\n",
    "expect=[[1,0],  [0,1],  [0,1], [1,0]] # ONE HOT REPRESENTATION! 'class' [1,0]==0 [0,1]==1\n",
    "\n",
    "# x = tf.Variable(x_)\n",
    "x = tf.placeholder(\"float\", [None,2]) #  can we feed directly?\n",
    "y_ = tf.placeholder(\"float\", [None, 2]) # two output classes\n",
    "\n",
    "number_hidden_nodes = 20 # 20 outputs to create some room for negatives and positives\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2, number_hidden_nodes], -.01, .01))\n",
    "b = tf.Variable(tf.random_uniform([number_hidden_nodes], -.01, .01))\n",
    "hidden  = tf.nn.relu(tf.matmul(x,W) + b) # first layer.\n",
    "\n",
    " # the XOR function is the first nontrivial function, for which a two layer network is needed.\n",
    "W2 = tf.Variable(tf.random_uniform([number_hidden_nodes,2], -.1, .1))\n",
    "b2 = tf.Variable(tf.zeros([2]))\n",
    "hidden2 = tf.matmul(hidden, W2)#+b2\n",
    "\n",
    "y = tf.nn.softmax(hidden2)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy)\n",
    "\n",
    "# Train\n",
    "tf.initialize_all_variables().run()\n",
    "for step in range(1000):\n",
    "    feed_dict={x: x_, y_:expect } # feed the net with our inputs and desired outputs.\n",
    "    e,a=sess.run([cross_entropy,train_step],feed_dict)\n",
    "    if e<1:break # early stopping yay\n",
    "    print (\"step %d : entropy %s\" % (step,e)) # error/loss should decrease over time\n",
    "\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) # argmax along dim-1\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # [True, False, True, True] -> [1,0,1,1] -> 0.75.\n",
    "\n",
    "print (\"accuracy %s\"%(accuracy.eval({x: x_, y_: expect})))\n",
    "\n",
    "learned_output=tf.argmax(y,1)\n",
    "print (learned_output.eval({x: x_}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
